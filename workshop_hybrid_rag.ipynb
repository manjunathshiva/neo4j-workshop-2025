{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üéì Workshop: Hybrid RAG - Graph + Vector Search with LangChain\n",
    "\n",
    "## Overview\n",
    "In this notebook, you'll learn how to build a **Hybrid RAG** system using LangChain that combines:\n",
    "1. **Graph RAG**: Structured knowledge from Neo4j (relationships, entities)\n",
    "2. **Vector RAG**: Semantic search using embeddings in Qdrant\n",
    "3. **Hybrid Retrieval**: Best of both worlds for superior answers\n",
    "\n",
    "## Why Hybrid RAG?\n",
    "- **Graph RAG**: Great for relationships, structured queries, multi-hop reasoning\n",
    "- **Vector RAG**: Excellent for semantic similarity, fuzzy matching\n",
    "- **Hybrid**: Combines precision of graphs with flexibility of vectors\n",
    "\n",
    "## Architecture\n",
    "```\n",
    "Document ‚Üí Entity Extraction ‚Üí Neo4j Graph (GraphCypherQAChain)\n",
    "        ‚Üí Chunking ‚Üí Embeddings ‚Üí Qdrant Vector DB (Similarity Search)\n",
    "                                      ‚Üì\n",
    "Query ‚Üí Graph Search + Vector Search ‚Üí Merge Results ‚Üí LLM Answer\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üì¶ Step 1: Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q neo4j>=5.15.0 qdrant-client>=1.7.0 sentence-transformers>=2.2.2 \\\n",
    "    torch>=2.1.0 langchain>=0.1.0 langchain-community>=0.0.10 langchain-groq>=1.0.0 \\\n",
    "    groq>=0.4.0 python-dotenv>=1.0.0 pydantic>=2.5.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîß Step 2: Setup Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "print(\"‚úÖ Environment variables loaded:\")\n",
    "print(f\"  - Neo4j URI: {os.getenv('NEO4J_URI')[:30]}...\")\n",
    "print(f\"  - Qdrant URL: {os.getenv('QDRANT_URL')[:30]}...\")\n",
    "print(f\"  - Groq API Key: {'‚úì Set' if os.getenv('GROQ_API_KEY') else '‚úó Missing'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìö Step 3: Load Sample Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/samples/university_research_network.md', 'r') as f:\n",
    "    document_text = f.read()\n",
    "\n",
    "print(f\"üìÑ Document loaded: {len(document_text)} characters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ü§ñ Step 4: Initialize LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "\n",
    "llm = ChatGroq(\n",
    "    groq_api_key=os.getenv('GROQ_API_KEY'),\n",
    "    model_name=\"moonshotai/kimi-k2-instruct-0905\",\n",
    "    temperature=0\n",
    ")\n",
    "\n",
    "print(\"‚úÖ LLM initialized (Moonshot AI Kimi K2)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìù Step 5: Chunk Document for Vector Embeddings\n",
    "\n",
    "Split document into smaller chunks for better semantic search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Split document into chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=50,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"]\n",
    ")\n",
    "\n",
    "chunks = text_splitter.split_text(document_text)\n",
    "\n",
    "print(f\"‚úÖ Created {len(chunks)} chunks\")\n",
    "print(f\"\\nExample chunk:\\n{chunks[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üßÆ Step 6: Generate Vector Embeddings\n",
    "\n",
    "Use sentence-transformers to create embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Initialize embedding model\n",
    "embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Generate embeddings for all chunks\n",
    "print(\"üîÑ Generating embeddings...\")\n",
    "embeddings = embedding_model.encode(chunks, show_progress_bar=True)\n",
    "\n",
    "print(f\"‚úÖ Generated {len(embeddings)} embeddings\")\n",
    "print(f\"   Embedding dimension: {embeddings[0].shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üíæ Step 7: Store Vectors in Qdrant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.models import Distance, VectorParams, PointStruct\n",
    "\n",
    "# Connect to Qdrant\n",
    "qdrant_client = QdrantClient(\n",
    "    url=os.getenv('QDRANT_URL'),\n",
    "    api_key=os.getenv('QDRANT_API_KEY')\n",
    ")\n",
    "\n",
    "collection_name = \"workshop_chunks\"\n",
    "\n",
    "# Clean up existing collection\n",
    "print(\"üßπ Cleaning up existing Qdrant collection...\")\n",
    "try:\n",
    "    qdrant_client.delete_collection(collection_name)\n",
    "    print(f\"   Deleted existing collection: {collection_name}\")\n",
    "except:\n",
    "    print(f\"   No existing collection found\")\n",
    "\n",
    "print(f\"\\nüì¶ Creating fresh collection: {collection_name}\")\n",
    "qdrant_client.create_collection(\n",
    "    collection_name=collection_name,\n",
    "    vectors_config=VectorParams(size=384, distance=Distance.COSINE)\n",
    ")\n",
    "print(\"‚úÖ Collection created!\")\n",
    "\n",
    "# Upload vectors\n",
    "points = [\n",
    "    PointStruct(\n",
    "        id=idx,\n",
    "        vector=embeddings[idx].tolist(),\n",
    "        payload={\"text\": chunks[idx]}\n",
    "    )\n",
    "    for idx in range(len(chunks))\n",
    "]\n",
    "\n",
    "qdrant_client.upsert(collection_name=collection_name, points=points)\n",
    "\n",
    "print(f\"‚úÖ Uploaded {len(points)} vectors to Qdrant\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîó Step 8: Build Knowledge Graph with LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.graphs import Neo4jGraph\n",
    "import json\n",
    "from groq import Groq\n",
    "\n",
    "# Connect to Neo4j\n",
    "graph = Neo4jGraph(\n",
    "    url=os.getenv('NEO4J_URI'),\n",
    "    username=os.getenv('NEO4J_USERNAME'),\n",
    "    password=os.getenv('NEO4J_PASSWORD')\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Connected to Neo4j\")\n",
    "\n",
    "# Clean up existing data\n",
    "print(\"\\nüßπ Cleaning up existing data...\")\n",
    "count_result = graph.query(\"MATCH (n) RETURN count(n) as node_count\")\n",
    "node_count = count_result[0]['node_count'] if count_result else 0\n",
    "print(f\"   Found {node_count} existing nodes\")\n",
    "graph.query(\"MATCH (n) DETACH DELETE n\")\n",
    "print(\"‚úÖ Database cleaned!\")\n",
    "\n",
    "# Extract entities\n",
    "groq_client = Groq(api_key=os.getenv('GROQ_API_KEY'))\n",
    "\n",
    "extraction_prompt = f\"\"\"Extract entities from this text. Return a JSON array.\n",
    "Each entity: name, type (UNIVERSITY, PERSON, RESEARCH_AREA), description.\n",
    "\n",
    "Text: {document_text}\n",
    "\n",
    "Return ONLY valid JSON array.\"\"\"\n",
    "\n",
    "response = groq_client.chat.completions.create(\n",
    "    model=\"moonshotai/kimi-k2-instruct-0905\",\n",
    "    messages=[{\"role\": \"user\", \"content\": extraction_prompt}],\n",
    "    temperature=0\n",
    ")\n",
    "\n",
    "entities = json.loads(response.choices[0].message.content)\n",
    "print(f\"‚úÖ Extracted {len(entities)} entities\")\n",
    "\n",
    "# Extract relationships\n",
    "relationship_prompt = f\"\"\"Extract relationships. Return JSON array.\n",
    "Each: source, target, type.\n",
    "\n",
    "Text: {document_text}\n",
    "\n",
    "Return ONLY valid JSON array.\"\"\"\n",
    "\n",
    "response = groq_client.chat.completions.create(\n",
    "    model=\"moonshotai/kimi-k2-instruct-0905\",\n",
    "    messages=[{\"role\": \"user\", \"content\": relationship_prompt}],\n",
    "    temperature=0\n",
    ")\n",
    "\n",
    "relationships = json.loads(response.choices[0].message.content)\n",
    "print(f\"‚úÖ Extracted {len(relationships)} relationships\")\n",
    "\n",
    "# Build graph\n",
    "import hashlib\n",
    "\n",
    "for entity in entities:\n",
    "    # Generate unique ID\n",
    "    entity_id = hashlib.md5(f\"{entity['name']}_{entity['type']}\".encode()).hexdigest()[:16]\n",
    "    \n",
    "    graph.query(\n",
    "        \"MERGE (e:Entity {id: $id, name: $name, type: $type}) SET e.description = $desc\",\n",
    "        params={\n",
    "            'id': entity_id,\n",
    "            'name': entity['name'],\n",
    "            'type': entity['type'],\n",
    "            'desc': entity.get('description', '')\n",
    "        }\n",
    "    )\n",
    "\n",
    "for rel in relationships:\n",
    "    try:\n",
    "        cypher = f\"\"\"\n",
    "        MATCH (s:Entity {{name: $source}})\n",
    "        MATCH (t:Entity {{name: $target}})\n",
    "        MERGE (s)-[:{rel['type'].upper().replace(' ', '_')}]->(t)\n",
    "        \"\"\"\n",
    "        graph.query(cypher, params={'source': rel['source'], 'target': rel['target']})\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "print(\"‚úÖ Knowledge graph built\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîç Step 9: Vector Search Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vector_search(query: str, top_k: int = 3):\n",
    "    \"\"\"Search Qdrant for similar chunks\"\"\"\n",
    "    query_embedding = embedding_model.encode(query)\n",
    "    \n",
    "    results = qdrant_client.search(\n",
    "        collection_name=collection_name,\n",
    "        query_vector=query_embedding.tolist(),\n",
    "        limit=top_k\n",
    "    )\n",
    "    \n",
    "    return [{\n",
    "        'text': hit.payload['text'],\n",
    "        'score': hit.score\n",
    "    } for hit in results]\n",
    "\n",
    "# Test vector search\n",
    "test_results = vector_search(\"artificial intelligence research\")\n",
    "print(\"üîç Vector search test:\")\n",
    "for i, r in enumerate(test_results, 1):\n",
    "    print(f\"\\n{i}. Score: {r['score']:.3f}\")\n",
    "    print(f\"   {r['text'][:150]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîó Step 10: Graph Search with GraphCypherQAChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import GraphCypherQAChain\n",
    "\n",
    "# Create graph QA chain\n",
    "graph.refresh_schema()\n",
    "\n",
    "graph_chain = GraphCypherQAChain.from_llm(\n",
    "    llm=llm,\n",
    "    graph=graph,\n",
    "    verbose=False,\n",
    "    allow_dangerous_requests=True\n",
    ")\n",
    "\n",
    "def graph_search(query: str):\n",
    "    \"\"\"Search graph using natural language\"\"\"\n",
    "    try:\n",
    "        result = graph_chain.invoke({\"query\": query})\n",
    "        return result['result']\n",
    "    except Exception as e:\n",
    "        return f\"Graph search error: {str(e)}\"\n",
    "\n",
    "# Test graph search\n",
    "test_result = graph_search(\"Tell me about Stanford University\")\n",
    "print(\"üîó Graph search test:\")\n",
    "print(test_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Step 11: Hybrid RAG - Combine Both!\n",
    "\n",
    "Merge graph and vector search results for comprehensive answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hybrid_rag(query: str):\n",
    "    \"\"\"Hybrid RAG: Combine graph + vector search\"\"\"\n",
    "    print(f\"‚ùì Query: {query}\\n\")\n",
    "    \n",
    "    # 1. Vector search\n",
    "    print(\"üîç Vector Search:\")\n",
    "    vector_results = vector_search(query, top_k=3)\n",
    "    vector_context = \"\\n\".join([r['text'] for r in vector_results])\n",
    "    for i, r in enumerate(vector_results, 1):\n",
    "        print(f\"  {i}. {r['text'][:100]}... (score: {r['score']:.3f})\")\n",
    "    \n",
    "    # 2. Graph search\n",
    "    print(\"\\nüîó Graph Search:\")\n",
    "    graph_context = graph_search(query)\n",
    "    print(f\"  {graph_context[:200]}...\")\n",
    "    \n",
    "    # 3. Combine contexts\n",
    "    combined_context = f\"\"\"Graph Knowledge:\n",
    "{graph_context}\n",
    "\n",
    "Vector Search Results:\n",
    "{vector_context}\"\"\"\n",
    "    \n",
    "    # 4. Generate answer\n",
    "    print(\"\\nüí° Generating hybrid answer...\")\n",
    "    response = groq_client.chat.completions.create(\n",
    "        model=\"moonshotai/kimi-k2-instruct-0905\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"Answer based on the provided context.\"},\n",
    "            {\"role\": \"user\", \"content\": f\"Context:\\n{combined_context}\\n\\nQuestion: {query}\"}\n",
    "        ],\n",
    "        temperature=0\n",
    "    )\n",
    "    \n",
    "    answer = response.choices[0].message.content\n",
    "    print(f\"\\n‚úÖ Hybrid Answer:\\n{answer}\")\n",
    "    return answer\n",
    "\n",
    "print(\"‚úÖ Hybrid RAG function ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üé™ Step 12: Try Different Queries!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query 1: Entity information\n",
    "hybrid_rag(\"Tell me about Stanford University\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query 2: Find connections\n",
    "hybrid_rag(\"Which entities are connected to Stanford University?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query 3: List collaborations\n",
    "hybrid_rag(\"How many collaborations do we have and list them?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query 4: List partnerships\n",
    "hybrid_rag(\"List all the Partnerships\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Step 13: Compare Search Methods\n",
    "\n",
    "Let's compare vector-only vs graph-only vs hybrid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_query = \"Tell me about Stanford University\"\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"COMPARISON: Vector vs Graph vs Hybrid\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Vector only\n",
    "print(\"\\n1Ô∏è‚É£ VECTOR SEARCH ONLY:\")\n",
    "print(\"-\" * 60)\n",
    "vector_results = vector_search(test_query, top_k=2)\n",
    "for r in vector_results:\n",
    "    print(f\"‚Ä¢ {r['text'][:150]}...\")\n",
    "\n",
    "# Graph only\n",
    "print(\"\\n2Ô∏è‚É£ GRAPH SEARCH ONLY:\")\n",
    "print(\"-\" * 60)\n",
    "graph_result = graph_search(test_query)\n",
    "print(graph_result)\n",
    "\n",
    "# Hybrid\n",
    "print(\"\\n3Ô∏è‚É£ HYBRID RAG:\")\n",
    "print(\"-\" * 60)\n",
    "hybrid_rag(test_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Key Takeaways\n",
    "\n",
    "### Vector Search Strengths:\n",
    "- ‚úÖ Semantic similarity matching\n",
    "- ‚úÖ Handles synonyms and paraphrasing\n",
    "- ‚úÖ Good for fuzzy queries\n",
    "- ‚úÖ Fast retrieval\n",
    "\n",
    "### Graph Search Strengths:\n",
    "- ‚úÖ Precise entity matching\n",
    "- ‚úÖ Relationship traversal\n",
    "- ‚úÖ Multi-hop reasoning\n",
    "- ‚úÖ Structured queries\n",
    "\n",
    "### Hybrid RAG Benefits:\n",
    "- üéØ **Best of both worlds**\n",
    "- üéØ More comprehensive context\n",
    "- üéØ Better answer quality\n",
    "- üéØ Handles diverse query types\n",
    "\n",
    "### LangChain Components Used:\n",
    "- ‚úÖ `Neo4jGraph`: Graph database connection\n",
    "- ‚úÖ `GraphCypherQAChain`: Natural language to Cypher\n",
    "- ‚úÖ `RecursiveCharacterTextSplitter`: Smart text chunking\n",
    "- ‚úÖ `ChatGroq`: LLM for generation\n",
    "\n",
    "---\n",
    "\n",
    "## üöÄ Next Steps\n",
    "\n",
    "- Experiment with different chunk sizes\n",
    "- Try other embedding models\n",
    "- Add more entity types to the graph\n",
    "- Implement weighted hybrid scoring\n",
    "- Deploy to production!\n",
    "\n",
    "---\n",
    "\n",
    "**Workshop Complete!** üéâ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {"name": "ipython", "version": 3},
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
